{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Imports"
      ],
      "metadata": {
        "id": "wk9lEw4Qxb6E"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Oax_oVA4xHRX"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.datasets import MNIST"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Set parameters and constants\n",
        "Chooses between the GPU (if available) or the CPU (default) to be used for training and testing the model.\n",
        "\n",
        "Set values for neural network hyperparameters.\n",
        "\n",
        "Set up the transformations to be used on the training data."
      ],
      "metadata": {
        "id": "WgMewl95z0_s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Choose device type (GPU or CPU), prioritizing GPU\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Hyperparameters\n",
        "epochs = 10\n",
        "batch_size = 64\n",
        "learning_rate = 0.001\n",
        "\n",
        "# Transformations for data augmentation\n",
        "# Maximum rotation of 20 degrees and shift of 10% of the image's size\n",
        "data_transform = transforms.Compose([\n",
        "    transforms.RandomAffine(degrees=20, translate=(0.1, 0.1)),\n",
        "    transforms.ToTensor()\n",
        "])"
      ],
      "metadata": {
        "id": "ms0T4YpxySom"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load training and testing data\n",
        "We will be training and testing our model with the MNIST dataset of handwritten numbers from 0 to 9. This dataset has been already split, where 60,000 images are for training and 10,000 images for testing.\n",
        "\n",
        "The training data will be transformed to randomly have up to 20 degrees of rotation and a maximum resizing of 10%. This will help the model deal with data that may not always be oriented correctly or have different sizes."
      ],
      "metadata": {
        "id": "Av_APdyM3owc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Training dataset with data augmentation\n",
        "train_dataset = MNIST(\n",
        "    root='./data', train=True, transform=data_transform, download=True\n",
        ")\n",
        "# Testing dataset\n",
        "test_dataset = MNIST(\n",
        "    root='./data', train=False, transform=transforms.ToTensor(), download=True\n",
        ")\n",
        "\n",
        "# Check datasets\n",
        "print('Train: ', len(train_dataset))    # 60,000\n",
        "print('Test:  ', len(test_dataset))     # 10,000\n",
        "\n",
        "# Create data loaders\n",
        "train_loader = DataLoader(\n",
        "    dataset=train_dataset, batch_size=batch_size, shuffle=True\n",
        ")\n",
        "test_loader = DataLoader(\n",
        "    dataset=test_dataset, batch_size=batch_size, shuffle=False\n",
        ")"
      ],
      "metadata": {
        "id": "0oeBAk4w3oCV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e8eafbb6-0233-4a6e-819a-5e765f22d883"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./data/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9912422/9912422 [00:00<00:00, 140141272.27it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/train-images-idx3-ubyte.gz to ./data/MNIST/raw\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 28881/28881 [00:00<00:00, 42955919.80it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/train-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1648877/1648877 [00:00<00:00, 43522997.80it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4542/4542 [00:00<00:00, 20074319.04it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Train:  60000\n",
            "Test:   10000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Neural network\n",
        "We use a Convolutional Neural Network because it's good at finding patterns in images, allowing us to classify the images."
      ],
      "metadata": {
        "id": "sTIRX-cZ0zmw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Convolutional Neural Network\n",
        "class ConvNeuralNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(ConvNeuralNet, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1)\n",
        "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
        "        self.fc1 = nn.Linear(64 * 7 * 7, 128)\n",
        "        self.fc2 = nn.Linear(128, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.conv1(x))\n",
        "        x = torch.max_pool2d(x, 2)\n",
        "        x = torch.relu(self.conv2(x))\n",
        "        x = torch.max_pool2d(x, 2)\n",
        "        x = x.view(-1, 64 * 7 * 7)\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        x = torch.softmax(x, dim=1)     # Softmax\n",
        "        return x"
      ],
      "metadata": {
        "id": "34K7pyft5wgq"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train the model with the MNIST dataset"
      ],
      "metadata": {
        "id": "BcNOKZ6bG3Ns"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize model and use chosen device\n",
        "model = ConvNeuralNet().to(device)\n",
        "\n",
        "# Define loss function and optimizer\n",
        "loss_function = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# Loop through each epoch and train the model\n",
        "for each_epoch in range(epochs):\n",
        "    model.train()\n",
        "    for batch_id, (data, target) in enumerate(train_loader):\n",
        "        data, target = torch.tensor(data).to(device), target.to(device)\n",
        "\n",
        "        # Zeroes out gradients\n",
        "        optimizer.zero_grad()\n",
        "        # Model predictions\n",
        "        output = model(data)\n",
        "        # Loss between prediction and actual targets\n",
        "        loss = loss_function(output, target)\n",
        "        # Backpropagation\n",
        "        loss.backward()\n",
        "        # Updates the model parameters with the Adam algorithm\n",
        "        optimizer.step()\n",
        "\n",
        "        if batch_id % 100 == 0:\n",
        "            print(\n",
        "                f'Epoch {each_epoch + 1}/{epochs}, '\n",
        "                f'Batch {batch_id}/{len(train_loader)}, '\n",
        "                f'Loss: {loss.item()}'\n",
        "            )\n",
        "\n",
        "# Save the trained model\n",
        "torch.save(model.state_dict(), 'mnist_model.pth')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DSCqWkZJ6INZ",
        "outputId": "90fc7c11-c878-4c27-8bdb-c8a4e7988e04"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-5-ef83927bc910>:12: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  data, target = torch.tensor(data).to(device), target.to(device)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10, Batch 0/938, Loss: 2.3024814128875732\n",
            "Epoch 1/10, Batch 100/938, Loss: 1.997850775718689\n",
            "Epoch 1/10, Batch 200/938, Loss: 1.7736642360687256\n",
            "Epoch 1/10, Batch 300/938, Loss: 1.7001923322677612\n",
            "Epoch 1/10, Batch 400/938, Loss: 1.6161185503005981\n",
            "Epoch 1/10, Batch 500/938, Loss: 1.6424939632415771\n",
            "Epoch 1/10, Batch 600/938, Loss: 1.7269768714904785\n",
            "Epoch 1/10, Batch 700/938, Loss: 1.7457095384597778\n",
            "Epoch 1/10, Batch 800/938, Loss: 1.6636097431182861\n",
            "Epoch 1/10, Batch 900/938, Loss: 1.5906940698623657\n",
            "Epoch 2/10, Batch 0/938, Loss: 1.630911946296692\n",
            "Epoch 2/10, Batch 100/938, Loss: 1.5668073892593384\n",
            "Epoch 2/10, Batch 200/938, Loss: 1.6112996339797974\n",
            "Epoch 2/10, Batch 300/938, Loss: 1.5971479415893555\n",
            "Epoch 2/10, Batch 400/938, Loss: 1.5463141202926636\n",
            "Epoch 2/10, Batch 500/938, Loss: 1.6359384059906006\n",
            "Epoch 2/10, Batch 600/938, Loss: 1.6689035892486572\n",
            "Epoch 2/10, Batch 700/938, Loss: 1.6769788265228271\n",
            "Epoch 2/10, Batch 800/938, Loss: 1.491653323173523\n",
            "Epoch 2/10, Batch 900/938, Loss: 1.5653098821640015\n",
            "Epoch 3/10, Batch 0/938, Loss: 1.6129926443099976\n",
            "Epoch 3/10, Batch 100/938, Loss: 1.5914396047592163\n",
            "Epoch 3/10, Batch 200/938, Loss: 1.535022497177124\n",
            "Epoch 3/10, Batch 300/938, Loss: 1.59432053565979\n",
            "Epoch 3/10, Batch 400/938, Loss: 1.5732117891311646\n",
            "Epoch 3/10, Batch 500/938, Loss: 1.6161648035049438\n",
            "Epoch 3/10, Batch 600/938, Loss: 1.560117244720459\n",
            "Epoch 3/10, Batch 700/938, Loss: 1.6230075359344482\n",
            "Epoch 3/10, Batch 800/938, Loss: 1.6365258693695068\n",
            "Epoch 3/10, Batch 900/938, Loss: 1.5595672130584717\n",
            "Epoch 4/10, Batch 0/938, Loss: 1.6120246648788452\n",
            "Epoch 4/10, Batch 100/938, Loss: 1.5879234075546265\n",
            "Epoch 4/10, Batch 200/938, Loss: 1.5628669261932373\n",
            "Epoch 4/10, Batch 300/938, Loss: 1.4915235042572021\n",
            "Epoch 4/10, Batch 400/938, Loss: 1.5227476358413696\n",
            "Epoch 4/10, Batch 500/938, Loss: 1.4616085290908813\n",
            "Epoch 4/10, Batch 600/938, Loss: 1.4975206851959229\n",
            "Epoch 4/10, Batch 700/938, Loss: 1.4921505451202393\n",
            "Epoch 4/10, Batch 800/938, Loss: 1.5162184238433838\n",
            "Epoch 4/10, Batch 900/938, Loss: 1.4835294485092163\n",
            "Epoch 5/10, Batch 0/938, Loss: 1.5574477910995483\n",
            "Epoch 5/10, Batch 100/938, Loss: 1.5363197326660156\n",
            "Epoch 5/10, Batch 200/938, Loss: 1.4785326719284058\n",
            "Epoch 5/10, Batch 300/938, Loss: 1.4979569911956787\n",
            "Epoch 5/10, Batch 400/938, Loss: 1.4975659847259521\n",
            "Epoch 5/10, Batch 500/938, Loss: 1.4838229417800903\n",
            "Epoch 5/10, Batch 600/938, Loss: 1.4984666109085083\n",
            "Epoch 5/10, Batch 700/938, Loss: 1.495368242263794\n",
            "Epoch 5/10, Batch 800/938, Loss: 1.4971083402633667\n",
            "Epoch 5/10, Batch 900/938, Loss: 1.4950727224349976\n",
            "Epoch 6/10, Batch 0/938, Loss: 1.5248284339904785\n",
            "Epoch 6/10, Batch 100/938, Loss: 1.5286476612091064\n",
            "Epoch 6/10, Batch 200/938, Loss: 1.4804221391677856\n",
            "Epoch 6/10, Batch 300/938, Loss: 1.481020212173462\n",
            "Epoch 6/10, Batch 400/938, Loss: 1.4888629913330078\n",
            "Epoch 6/10, Batch 500/938, Loss: 1.4667737483978271\n",
            "Epoch 6/10, Batch 600/938, Loss: 1.5309617519378662\n",
            "Epoch 6/10, Batch 700/938, Loss: 1.4760899543762207\n",
            "Epoch 6/10, Batch 800/938, Loss: 1.4881346225738525\n",
            "Epoch 6/10, Batch 900/938, Loss: 1.4839354753494263\n",
            "Epoch 7/10, Batch 0/938, Loss: 1.4898794889450073\n",
            "Epoch 7/10, Batch 100/938, Loss: 1.4771541357040405\n",
            "Epoch 7/10, Batch 200/938, Loss: 1.4718164205551147\n",
            "Epoch 7/10, Batch 300/938, Loss: 1.4613789319992065\n",
            "Epoch 7/10, Batch 400/938, Loss: 1.4799810647964478\n",
            "Epoch 7/10, Batch 500/938, Loss: 1.4830760955810547\n",
            "Epoch 7/10, Batch 600/938, Loss: 1.5369091033935547\n",
            "Epoch 7/10, Batch 700/938, Loss: 1.4923983812332153\n",
            "Epoch 7/10, Batch 800/938, Loss: 1.4940390586853027\n",
            "Epoch 7/10, Batch 900/938, Loss: 1.494842529296875\n",
            "Epoch 8/10, Batch 0/938, Loss: 1.4954174757003784\n",
            "Epoch 8/10, Batch 100/938, Loss: 1.4929219484329224\n",
            "Epoch 8/10, Batch 200/938, Loss: 1.5139734745025635\n",
            "Epoch 8/10, Batch 300/938, Loss: 1.4981049299240112\n",
            "Epoch 8/10, Batch 400/938, Loss: 1.504434585571289\n",
            "Epoch 8/10, Batch 500/938, Loss: 1.4766730070114136\n",
            "Epoch 8/10, Batch 600/938, Loss: 1.4873089790344238\n",
            "Epoch 8/10, Batch 700/938, Loss: 1.484713077545166\n",
            "Epoch 8/10, Batch 800/938, Loss: 1.4917237758636475\n",
            "Epoch 8/10, Batch 900/938, Loss: 1.5002400875091553\n",
            "Epoch 9/10, Batch 0/938, Loss: 1.4901106357574463\n",
            "Epoch 9/10, Batch 100/938, Loss: 1.486724853515625\n",
            "Epoch 9/10, Batch 200/938, Loss: 1.4798688888549805\n",
            "Epoch 9/10, Batch 300/938, Loss: 1.4778263568878174\n",
            "Epoch 9/10, Batch 400/938, Loss: 1.5081281661987305\n",
            "Epoch 9/10, Batch 500/938, Loss: 1.4613507986068726\n",
            "Epoch 9/10, Batch 600/938, Loss: 1.4867427349090576\n",
            "Epoch 9/10, Batch 700/938, Loss: 1.4636986255645752\n",
            "Epoch 9/10, Batch 800/938, Loss: 1.5534135103225708\n",
            "Epoch 9/10, Batch 900/938, Loss: 1.4696742296218872\n",
            "Epoch 10/10, Batch 0/938, Loss: 1.497390627861023\n",
            "Epoch 10/10, Batch 100/938, Loss: 1.4805561304092407\n",
            "Epoch 10/10, Batch 200/938, Loss: 1.4791104793548584\n",
            "Epoch 10/10, Batch 300/938, Loss: 1.4819409847259521\n",
            "Epoch 10/10, Batch 400/938, Loss: 1.4782764911651611\n",
            "Epoch 10/10, Batch 500/938, Loss: 1.4714488983154297\n",
            "Epoch 10/10, Batch 600/938, Loss: 1.477765679359436\n",
            "Epoch 10/10, Batch 700/938, Loss: 1.5258022546768188\n",
            "Epoch 10/10, Batch 800/938, Loss: 1.4612098932266235\n",
            "Epoch 10/10, Batch 900/938, Loss: 1.50730299949646\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load the saved model for testing"
      ],
      "metadata": {
        "id": "UUlSsIoXG_P3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the saved model for testing\n",
        "model.load_state_dict(torch.load('mnist_model.pth'))\n",
        "model.eval()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "49ZmIClVHDIl",
        "outputId": "6f317130-56f7-42a7-ff88-3a75e0130306"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ConvNeuralNet(\n",
              "  (conv1): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "  (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "  (fc1): Linear(in_features=3136, out_features=128, bias=True)\n",
              "  (fc2): Linear(in_features=128, out_features=10, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Test the model for accuracy, using the test dataset"
      ],
      "metadata": {
        "id": "SVfWaQ5gIKzy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "total_tests = 0\n",
        "correct_pred = 0\n",
        "with torch.no_grad():\n",
        "    for data, target in test_loader:\n",
        "        data, target = data.clone().detach().to(device), target.to(device)\n",
        "        output = model(data)\n",
        "        _, prediction = torch.max(output.data, 1)\n",
        "        total_tests += target.size(0)\n",
        "        correct_pred += (prediction == target).sum().item()\n",
        "\n",
        "model_accuracy = correct_pred / total_tests\n",
        "print(f'Accuracy on the test set: {model_accuracy * 100:.2f}%')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yRWRIIxPHn_-",
        "outputId": "91ae7153-e2e1-447c-b26e-1067d8f99d20"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy on the test set: 99.11%\n"
          ]
        }
      ]
    }
  ]
}